# -*- coding: utf-8 -*-
"""
Created on Mon Mar 14 16:13:24 2016

@author: lifu
"""

from liflib2.dl import SoftmaxMLP
from liflib2.dl import DataSet
import liflib2
import numpy as np
import argparse
import timeit
import matplotlib.pyplot as plt

if __name__ == '__main__':
    prog_name = 'Softmax MLP Demo'
    description = 'This is a demo program for SoftmaxMLP in liflib.dl.'
    
    ap = argparse.ArgumentParser(description = description, prog = prog_name)
         
    ap.add_argument('-i', '--input', dest = 'dataset', action = 'store', 
                    default = 'dataset.dat', 
                    help = 'standard dataset file generated by liflib utility')
    ap.add_argument('-t', '--tol', action = 'store', dest = 'tol', 
                    type = float, default = 1e-5,
                    help = 'tolerance for testing convergence [default:1e-5]')
    ap.add_argument('-b', '--batch', action = 'store', dest = 'batch_size',
                    type = int, default = 50,
                    help = 'batch size when doing mini-batch gradient descent [default:50]') 
    ap.add_argument('-r', '--random', action = 'store', dest = 'random', 
                    type = bool, default = True,
                    help = 'whether randomly selecting training samples [default:True]')                                
    ap.add_argument('-m', '--maxiters', action = 'store', type = int,
                    dest = 'max_iters', default = None,
                    help = 'output test result to specific path [default:None]')     
    ap.add_argument('-s', '--step', action = 'store', type = float,
                    dest = 'step', default = 1.0,
                    help = 'step size of sgd [default:1.0]')
    ap.add_argument('-d', '--display', action = 'store', dest = 'display',
                    type = int, default = 100,
                    help = 'display information every period of time [default:100]') 
    ap.add_argument('-v', '--save', action = 'store', dest = 'save',
                    type = int, default = 10000,
                    help = 'save progress to cache files every period of time [default:10000]') 
    ap.add_argument('-l', '--layer', action = 'append', dest = 'layer',
                    type = int,
                    help = 'add one hidden layer with supplied size') 
    ap.add_argument('-p', '--plot', action = 'store_true', dest = 'plot',
                    help = 'plot cost function') 
                    
    ap.add_argument('--adagrad', action = 'store_true', dest = 'adagrad',
                    help = 'use adagrad')
    ap.add_argument('--anneal', action = 'store', dest = 'anneal', type = int,
                    help = 'decrease step size every period of time [default: None]') 
    ap.add_argument('--l2', action = 'store', dest = 'l2',
                    type = float, default = 0, 
                    help = 'specify the coefficient of L2 regularization term. [default: 0]')
    ap.add_argument('--test', action = 'store_true', dest = 'test',
                    help = 'test precision')
    ap.add_argument('--check', action = 'store_true', dest = 'check',
                    help = 'do gradient check without training')
    args = ap.parse_args()

    dataset = DataSet.load(args.dataset) 
    print 'Data loaded!\n(n_samples = %d, n_features = %d, n_label_classes = %d)' % (dataset.n_samples, 
                                                                                  dataset.n_features, 
                                                                                  dataset.n_label_classes)
    
    layers = [dataset.n_features]
    if args.layer:
        for h in args.layer:
            layers.append(h)
    else:
        layers.append(max(dataset.n_features, dataset.n_label_classes))
    layers.append(dataset.n_label_classes)
    nn = SoftmaxMLP(tuple(layers))    
    print 'A %d-layer neural network with layer size %s has been constructed.' % (nn.n_layers, nn.layer_size)
    
    if args.check:    
        nn.random_init()
        liflib2.gradcheck_naive(lambda x: nn.objective(x, dataset, 10, 
                                                         regularization = 'l2', 
                                                         _lambda = 0.1), 
                                nn.get_parameters(), 
                                verbose = True)    
    elif args.test:
        nn.fit(dataset, f_min_options = {'max_iters': 0})
        n_correct = 0
        for i in xrange(dataset.n_training_samples):
            features, label = dataset.get_training_sample()
            if np.argmax(nn.predict(features)) == label:
                n_correct += 1
                
        print 'For training set: total: %d, correct: %d, precision = %g%%' % (dataset.n_training_samples, n_correct, n_correct * 100.0 / dataset.n_training_samples)
        
        n_correct = 0
        for i in xrange(dataset.n_test_samples):
            features, label = dataset.get_test_sample()
            if np.argmax(nn.predict(features)) == label:
                n_correct += 1
                
        print 'For test set: total: %d, correct: %d, precision = %g%%' % (dataset.n_test_samples, n_correct, n_correct * 100.0 / dataset.n_test_samples)
    else:     
        obj_opt={'batch_size': args.batch_size, 
                'randomized':args.random}
        if args.l2:            
            obj_opt['regularization'] = 'l2'
            obj_opt['_lambda'] = args.l2
            

        old_cost = None
        min_cost = float('inf')
        max_cost = float('-inf')
    
        if args.plot:
            plt.ion()
            plt.axis([0, 500, 0, 5])
            plt.show()
        
        def callback(state):
            if state.it % args.save == 0:
                liflib2.save_iter_state(state)
            if state.it % args.display == 0:
                print 'Iterateion %d: cost = %g' % (state.it, state.cost)
                if args.plot:
                    global old_cost, min_cost, max_cost
                    #adjust axis
                    while state.it * 1.2 > plt.xlim()[1]:
                            plt.xlim(xmax = state.it * 1.2)
                    if state.cost >= plt.ylim()[1]:
                        plt.ylim(ymax = state.cost * 1.05)
                    # plot min/max point
                    if state.cost < min_cost:
                        plt.plot(state.it, state.cost, 'go')
                        min_cost = state.cost
                    elif state.cost > max_cost:                    
                        plt.plot(state.it, state.cost, 'ro')
                        max_cost = state.cost
                    # plot title
                    plt.title('cost: %g, min: %g' % (state.cost, min_cost))                
                    # plot lines
                    if old_cost:
                        line = 'r-' if state.cost > old_cost else 'g-'
                        plt.plot([state.it - args.display, state.it], 
                                 [old_cost, state.cost], line)
                    plt.draw()
                    plt.pause(0.1)
                    # record old cost
                    old_cost = state.cost
            
        f_min_opt = {'max_iters': args.max_iters,
                     'tol': args.tol, 
                     'step_size': args.step, 
                     'use_save': True,
                     'max_iters': args.max_iters,
                     'callback': callback,
                     'anneal_every': args.anneal,
                     'use_adagrad': args.adagrad}
                     
        if args.adagrad and args.anneal:
            print 'WARNING: both annealing and adagrad are turned on.'
            
        try:
            t1 = timeit.time.time()
            nn.fit(dataset, obj_options = obj_opt, f_min_options = f_min_opt)
        except KeyboardInterrupt:
            print 'Terminated by key interrupt'
        finally:
            t2 = timeit.time.time()        
            print 'Training process last for %g seconds in total' % (t2 - t1)
        
